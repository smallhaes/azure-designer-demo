{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Customized Modules to Create an Azure Machine Learning Pipeline\n",
    "\n",
    "In this notebook, we will demonstrate how to use customized modules to create an Azure Machine Learning Pipeline which is associated with the [fastText](https://fasttext.cc/) algorithm. Customized modules are created with the extension azure-cli-ml of Azure CLI. \n",
    "\n",
    "If you don't have the input dataset, you could prepare your it with the instructions in ```prepare_data.ipynb```. \n",
    "\n",
    "The outline of this notebook is as follows:\n",
    "\n",
    "- Upload the dataset onto a blob container and register it to the workspace.\n",
    "- Register an anonymous module from yaml file to workspace.\n",
    "- Construct the pipeline.\n",
    "- Visualize and run the pipeline.\n",
    "\n",
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at https://github.com/Azure/MachineLearningNotebooks first. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset, Datastore, Workspace\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.pipeline.wrapper import Module, dsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "Create a workspace object from the existing workspace. Workspace.from_config() reads the file config.json and loads the details into an object named workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DesignerDRI_EASTUS\n",
      "DesignerDRI\n",
      "eastus\n",
      "74eccef0-4b8d-4f83-b5f9-fa100d155b22\n",
      "dict_keys(['aks-dev', 'aks-dev2', 'attached-aks', 'default', 'compute', 'cpu-cluster', 'aml-compute'])\n"
     ]
    }
   ],
   "source": [
    "workspace = Workspace.from_config('config.json')\n",
    "print(workspace.name, workspace.resource_group, workspace.location, workspace.subscription_id,\n",
    "      workspace.compute_targets.keys(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes. If the AmlCompute with that name is already in your workspace the code will skip the creation process.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target: aml-compute\n"
     ]
    }
   ],
   "source": [
    "aml_compute_name = 'aml-compute'\n",
    "if aml_compute_name in workspace.compute_targets:\n",
    "    aml_compute = AmlCompute(workspace, aml_compute_name)\n",
    "    print(\"Found existing compute target: {}\".format(aml_compute_name))\n",
    "else:\n",
    "    print(\"Creating new compute target: {}\".format(aml_compute_name))\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\", min_nodes=1, max_nodes=4)\n",
    "    aml_compute = ComputeTarget.create(workspace, aml_compute_name, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the dataset onto a blob container and register it to the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = \"THUCNews\"\n",
    "# if the workspace don't contain the dataset, then register it\n",
    "if not dataset_name in workspace.datasets:\n",
    "    # upload files in 'data' to a blob container.\n",
    "    # our files are in the directory of 'path_on_datastore' in the blob container.\n",
    "    path_on_datastore = 'my_dataset'\n",
    "    datastore = Datastore.get(workspace=workspace, datastore_name='workspaceblobstore')\n",
    "    datastore.upload(src_dir='data', target_path=path_on_datastore, overwrite=True, show_progress=True)\n",
    "    description = 'THUCNews dataset is generated by filtering and filtering historical data \\\n",
    "    of Sina News RSS subscription channel from 2005 to 2011'\n",
    "    # get the DataPath object associated with the uploaded dataset.\n",
    "    datastore_path = [DataPath(datastore=datastore, path_on_datastore=path_on_datastore)]\n",
    "    data = Dataset.File.from_files(path=datastore_path)\n",
    "    # register the dataset to your workspace.\n",
    "    data.register(workspace=workspace, name=dataset_name, description=description, create_new_version=True)\n",
    "dataset = workspace.datasets[dataset_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register an anonymous module from yaml file to the workspace.\n",
    "If you decorate your module function with ```@dsl.module```, azure-cli-ml could help to generate the ```*.spec.yaml``` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_data_txt_module_func = Module.from_yaml(workspace, 'split_data_txt/split_data_txt.spec.yaml')\n",
    "fasttext_train_module_func = Module.from_yaml(workspace, 'fasttext_train/fasttext_train.spec.yaml')\n",
    "fasttext_evaluation_module_func = Module.from_yaml(workspace, 'fasttext_evaluation/fasttext_evaluation.spec.yaml')\n",
    "compare_two_models_module_func = Module.from_yaml(workspace, 'compare_two_models/compare_two_models.spec.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Construct the pipeline\n",
    "our pipeline contains two sub pipelines. They represent two training processes of the fastText model with different parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sub pipeline\n",
    "@dsl.pipeline(name='sub_pipeline', description='A sub pipeline including processes of data processing/train/evaluation',\n",
    "              default_compute_target=aml_compute_name)\n",
    "def training_pipeline(epochs):\n",
    "    split_data_txt = split_data_txt_module_func(\n",
    "        input_dir=dataset,\n",
    "        training_data_ratio=0.1,\n",
    "        validation_data_ratio=0.2\n",
    "    )\n",
    "    fasttext_train = fasttext_train_module_func(\n",
    "        training_data_dir=split_data_txt.outputs.training_data_output,\n",
    "        validation_data_dir=split_data_txt.outputs.validation_data_output,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    fasttext_evaluation = fasttext_evaluation_module_func(\n",
    "        trained_model_dir=fasttext_train.outputs.trained_model_dir,\n",
    "        test_data_dir=split_data_txt.outputs.test_data_output\n",
    "    )\n",
    "\n",
    "    return {**fasttext_evaluation.outputs, **fasttext_train.outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name='fasttext_pipeline',\n",
    "              description='The pipeline that trains two fasttext models and output the better one',\n",
    "              default_compute_target=aml_compute_name)\n",
    "def fasttext_pipeline():\n",
    "    train_and_evalute_model1 = training_pipeline(epochs=3)\n",
    "    train_and_evalute_model2 = training_pipeline(epochs=6)\n",
    "    compare = compare_two_models_module_func(\n",
    "        first_trained_model=train_and_evalute_model1.outputs.trained_model_dir,\n",
    "        first_trained_result=train_and_evalute_model1.outputs.model_testing_result,\n",
    "        second_trained_model=train_and_evalute_model2.outputs.trained_model_dir,\n",
    "        second_trained_result=train_and_evalute_model2.outputs.model_testing_result\n",
    "    )\n",
    "    return {**compare.outputs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualize and run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the pipeline\n",
    "pipeline = fasttext_pipeline()\n",
    "# save the pipeline if necessary\n",
    "# pipeline.save(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the pipeline\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# run the pipeline\n",
    "experiment_name = 'fasttext_pipeline'\n",
    "# regenerate_outputs indicates whether to force regeneration of all step outputs and disallow data reuse for this run\n",
    "# if regenerate_outputs is False, this run may reuse results from previous runs and subsequent runs may reuse the results of this run\n",
    "pipeline_run = pipeline.submit(experiment_name=experiment_name, regenerate_outputs=False)\n",
    "# wait_for_completion() visualize the execution process of the pipeline\n",
    "# you could also view this process on Azure Machine Learning Portal\n",
    "pipeline_run.wait_for_completion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}