{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using Azure Machine Learning Pipelines for Batch Inference\n",
    "\n",
    "In this notebook, we will demonstrate how to make predictions on large quantities of data asynchronously using the ML pipelines with Azure Machine Learning. Batch inference (or batch scoring) provides cost-effective inference, with unparalleled throughput for asynchronous applications. Batch prediction pipelines can scale to perform inference on terabytes of production data. Batch prediction is optimized for high throughput, fire-and-forget predictions for a large collection of data.\n",
    "\n",
    "> **Tip**\n",
    "If your system requires low-latency processing (to process a single document or small set of documents quickly), use [real-time scoring](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-consume-web-service) instead of batch prediction.\n",
    "\n",
    "In this example will be take a digit identification model already-trained on MNIST dataset using the [AzureML training with deep learning example notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-keras/train-hyperparameter-tune-deploy-with-keras.ipynb), and run that trained model on some of the MNIST test images in batch.\n",
    "\n",
    "The input dataset used for this notebook differs from a standard MNIST dataset in that it has been converted to PNG images to demonstrate use of files as inputs to Batch Inference. A sample of PNG-converted images of the MNIST dataset were take from [this repository](https://github.com/myleott/mnist_png).\n",
    "\n",
    "The outline of this notebook is as follows:\n",
    "\n",
    "- Create a DataStore referencing MNIST images stored in a blob container.\n",
    "- Register the pretrained MNIST model into the model registry.\n",
    "- Use the registered model to do batch inference on the images in the data blob container.\n",
    "\n",
    "## Prerequisites\n",
    "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at https://github.com/Azure/MachineLearningNotebooks first. This sets you up with a working config file that has information on your workspace, subscription id, etc.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "from azureml.data.datapath import DataPath\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create word_to_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = 'data/data.txt'\n",
    "word_count = {}\n",
    "index = 2\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        text = line.split('\\t')[0]\n",
    "        for word in text.split(' '):\n",
    "            if not word in word_count:\n",
    "                word_count[word] = 0\n",
    "            word_count[word] += 1\n",
    "word_count_list = sorted(word_count.items(), key=lambda x : x[1], reverse=True)\n",
    "\n",
    "word_to_index = {\"[PAD]\": 0, \"[UNK]\": 1}\n",
    "vocab_size = 5000\n",
    "index = 2\n",
    "for w_c in word_count_list:\n",
    "    if index == vocab_size:\n",
    "        break\n",
    "    word = w_c[0]\n",
    "    count = w_c[1]\n",
    "    word_to_index[word] = index\n",
    "    index +=1\n",
    "with open('data/word_to_index.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(word_to_index, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create a datastore containing sample images\n",
    "The input dataset used for this notebook differs from a standard MNIST dataset in that it has been converted to PNG images to demonstrate use of files as inputs to Batch Inference. A sample of PNG-converted images of the MNIST dataset were take from [this repository](https://github.com/myleott/mnist_png).\n",
    "\n",
    "We have created a public blob container `sampledata` on an account named `pipelinedata`, containing these images from the MNIST dataset. In the next step, we create a datastore with the name `images_datastore`, which points to this blob container. In the call to `register_azure_blob_container` below, setting the `overwrite` flag to `True` overwrites any datastore that was created previously with that name.\n",
    "\n",
    "This step can be changed to point to your blob container by providing your own `datastore_name`, `container_name`, and `account_name`.\n",
    "\n",
    "\n",
    "\n",
    "### Prepare data for batch inference\n",
    "\n",
    "we need to create a file for each input sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = 'data/data.txt'\n",
    "dir_ = 'data_for_batch_inference'\n",
    "os.makedirs(dir_, exist_ok=True)\n",
    "num = 200\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    lines = []\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        if i==num:\n",
    "            break\n",
    "        lines.append(line.split('\\t')[0])\n",
    "for i, line in enumerate(lines):\n",
    "    path = os.path.join(dir_, str(i))\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Workspace.create(name='fundamental3', subscription_id='4f455bd0-f95a-4b7d-8d08-078611508e0b', resource_group='fundamental')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace = Workspace.from_config('config.json')\n",
    "workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 4 files\n",
      "Uploading data/azureml/358a3e99-f299-4089-b2cf-cecc32ac34f8/Trained_model_dir/BestModel\n",
      "Uploading data/data.txt\n",
      "Uploading data/label.txt\n",
      "Uploading data/word_to_index.json\n",
      "Uploaded data/label.txt, 1 files out of an estimated total of 4\n",
      "Uploaded data/azureml/358a3e99-f299-4089-b2cf-cecc32ac34f8/Trained_model_dir/BestModel, 2 files out of an estimated total of 4\n",
      "Uploaded data/word_to_index.json, 3 files out of an estimated total of 4\n",
      "Uploaded data/data.txt, 4 files out of an estimated total of 4\n",
      "Uploaded 4 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_451c7d517a0a42188a394042cb895dfa"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_on_datastore = 'my_dataset'\n",
    "datastore = Datastore.get(workspace=workspace, datastore_name='workspaceblobstore')\n",
    "datastore.upload(src_dir='data', target_path=path_on_datastore, overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Register the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'my_dataset')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"f2d71ae3-678a-4673-9485-56c56e2e1389\",\n",
       "    \"name\": \"THUCNews\",\n",
       "    \"version\": 1,\n",
       "    \"description\": \"THUCNews dataset is generated by filtering and filtering historical data of Sina News RSS subscription channel from 2005 to 2011\",\n",
       "    \"workspace\": \"Workspace.create(name='fundamental3', subscription_id='4f455bd0-f95a-4b7d-8d08-078611508e0b', resource_group='fundamental')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'THUCNews'\n",
    "description = 'THUCNews dataset is generated by filtering and filtering historical data \\\n",
    "of Sina News RSS subscription channel from 2005 to 2011'\n",
    "datastore_path = [DataPath(datastore=datastore, path_on_datastore=path_on_datastore)]\n",
    "data = Dataset.File.from_files(path=datastore_path)\n",
    "data.register(workspace=workspace, name=dataset_name, description=description, create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 200 files\n",
      "Uploading data_for_batch_inference/0\n",
      "Uploading data_for_batch_inference/1\n",
      "Uploading data_for_batch_inference/10\n",
      "Uploading data_for_batch_inference/100\n",
      "Uploading data_for_batch_inference/101\n",
      "Uploading data_for_batch_inference/102\n",
      "Uploading data_for_batch_inference/103\n",
      "Uploading data_for_batch_inference/104\n",
      "Uploading data_for_batch_inference/105\n",
      "Uploading data_for_batch_inference/106\n",
      "Uploading data_for_batch_inference/107\n",
      "Uploading data_for_batch_inference/108\n",
      "Uploading data_for_batch_inference/109\n",
      "Uploading data_for_batch_inference/11\n",
      "Uploading data_for_batch_inference/110\n",
      "Uploading data_for_batch_inference/111\n",
      "Uploading data_for_batch_inference/112\n",
      "Uploading data_for_batch_inference/113\n",
      "Uploading data_for_batch_inference/114\n",
      "Uploading data_for_batch_inference/115\n",
      "Uploading data_for_batch_inference/116\n",
      "Uploading data_for_batch_inference/117\n",
      "Uploading data_for_batch_inference/118\n",
      "Uploading data_for_batch_inference/119\n",
      "Uploading data_for_batch_inference/12\n",
      "Uploading data_for_batch_inference/120\n",
      "Uploading data_for_batch_inference/121\n",
      "Uploading data_for_batch_inference/122\n",
      "Uploading data_for_batch_inference/123\n",
      "Uploading data_for_batch_inference/124\n",
      "Uploading data_for_batch_inference/125\n",
      "Uploading data_for_batch_inference/126\n",
      "Uploaded data_for_batch_inference/10, 1 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/127\n",
      "Uploaded data_for_batch_inference/0, 2 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/128\n",
      "Uploaded data_for_batch_inference/100, 3 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/129\n",
      "Uploaded data_for_batch_inference/105, 4 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/13\n",
      "Uploaded data_for_batch_inference/107, 5 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/130\n",
      "Uploaded data_for_batch_inference/108, 6 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/131\n",
      "Uploaded data_for_batch_inference/126, 7 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/132\n",
      "Uploaded data_for_batch_inference/11, 8 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/1, 9 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/133\n",
      "Uploaded data_for_batch_inference/129, 10 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/134\n",
      "Uploaded data_for_batch_inference/13, 11 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/135\n",
      "Uploaded data_for_batch_inference/116, 12 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/136\n",
      "Uploading data_for_batch_inference/137\n",
      "Uploaded data_for_batch_inference/127, 13 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/128, 14 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/138\n",
      "Uploaded data_for_batch_inference/114, 15 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/139\n",
      "Uploading data_for_batch_inference/14\n",
      "Uploaded data_for_batch_inference/131, 16 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/140\n",
      "Uploaded data_for_batch_inference/104, 17 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/141\n",
      "Uploaded data_for_batch_inference/101, 18 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/142\n",
      "Uploaded data_for_batch_inference/130, 19 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/143\n",
      "Uploaded data_for_batch_inference/123, 20 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/144\n",
      "Uploaded data_for_batch_inference/110, 21 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/103, 22 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/145\n",
      "Uploaded data_for_batch_inference/109, 23 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/146\n",
      "Uploaded data_for_batch_inference/121, 24 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/147\n",
      "Uploaded data_for_batch_inference/132, 25 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/148\n",
      "Uploading data_for_batch_inference/149\n",
      "Uploaded data_for_batch_inference/111, 26 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/15\n",
      "Uploaded data_for_batch_inference/113, 27 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/150\n",
      "Uploaded data_for_batch_inference/118, 28 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/120, 29 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/151\n",
      "Uploaded data_for_batch_inference/119, 30 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/152\n",
      "Uploading data_for_batch_inference/153\n",
      "Uploaded data_for_batch_inference/112, 31 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/154\n",
      "Uploaded data_for_batch_inference/106, 32 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/155\n",
      "Uploaded data_for_batch_inference/124, 33 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/156\n",
      "Uploaded data_for_batch_inference/115, 34 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/157\n",
      "Uploaded data_for_batch_inference/117, 35 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/158\n",
      "Uploaded data_for_batch_inference/139, 36 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/159\n",
      "Uploaded data_for_batch_inference/133, 37 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/16\n",
      "Uploaded data_for_batch_inference/122, 38 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/102, 39 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/160\n",
      "Uploaded data_for_batch_inference/125, 40 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/12, 41 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/161\n",
      "Uploading data_for_batch_inference/162\n",
      "Uploaded data_for_batch_inference/134, 42 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/163\n",
      "Uploaded data_for_batch_inference/141, 43 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/164\n",
      "Uploaded data_for_batch_inference/137, 44 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/165\n",
      "Uploading data_for_batch_inference/166\n",
      "Uploaded data_for_batch_inference/150, 45 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/167\n",
      "Uploaded data_for_batch_inference/152, 46 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/143, 47 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/168\n",
      "Uploaded data_for_batch_inference/155, 48 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/169\n",
      "Uploaded data_for_batch_inference/153, 49 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/17\n",
      "Uploaded data_for_batch_inference/158, 50 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/170\n",
      "Uploaded data_for_batch_inference/147, 51 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/171\n",
      "Uploaded data_for_batch_inference/145, 52 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/172\n",
      "Uploading data_for_batch_inference/173\n",
      "Uploaded data_for_batch_inference/15, 53 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/156, 54 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/174\n",
      "Uploaded data_for_batch_inference/140, 55 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/175\n",
      "Uploaded data_for_batch_inference/136, 56 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/176\n",
      "Uploaded data_for_batch_inference/138, 57 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/177\n",
      "Uploaded data_for_batch_inference/149, 58 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/178\n",
      "Uploaded data_for_batch_inference/16, 59 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/179\n",
      "Uploading data_for_batch_inference/18\n",
      "Uploaded data_for_batch_inference/146, 60 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/180\n",
      "Uploaded data_for_batch_inference/159, 61 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/181\n",
      "Uploaded data_for_batch_inference/151, 62 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/182\n",
      "Uploaded data_for_batch_inference/154, 63 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/135, 64 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/183\n",
      "Uploaded data_for_batch_inference/14, 65 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/184\n",
      "Uploading data_for_batch_inference/185\n",
      "Uploaded data_for_batch_inference/157, 66 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/186\n",
      "Uploaded data_for_batch_inference/144, 67 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/187\n",
      "Uploaded data_for_batch_inference/163, 68 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/161, 69 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/188\n",
      "Uploaded data_for_batch_inference/162, 70 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/189\n",
      "Uploading data_for_batch_inference/19\n",
      "Uploaded data_for_batch_inference/170, 71 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/190\n",
      "Uploaded data_for_batch_inference/168, 72 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/191\n",
      "Uploaded data_for_batch_inference/174, 73 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/148, 74 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/192\n",
      "Uploaded data_for_batch_inference/160, 75 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/180, 76 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/193\n",
      "Uploading data_for_batch_inference/194\n",
      "Uploaded data_for_batch_inference/182, 77 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/195\n",
      "Uploaded data_for_batch_inference/142, 78 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/196\n",
      "Uploaded data_for_batch_inference/177, 79 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/197\n",
      "Uploaded data_for_batch_inference/173, 80 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/198\n",
      "Uploaded data_for_batch_inference/172, 81 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data_for_batch_inference/171, 82 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/2\n",
      "Uploaded data_for_batch_inference/169, 83 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/20\n",
      "Uploading data_for_batch_inference/21\n",
      "Uploaded data_for_batch_inference/185, 84 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/22\n",
      "Uploaded data_for_batch_inference/18, 85 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/179, 86 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/23\n",
      "Uploading data_for_batch_inference/24\n",
      "Uploaded data_for_batch_inference/181, 87 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/25\n",
      "Uploaded data_for_batch_inference/188, 88 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/26\n",
      "Uploaded data_for_batch_inference/183, 89 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/27\n",
      "Uploaded data_for_batch_inference/178, 90 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/28\n",
      "Uploaded data_for_batch_inference/184, 91 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/29\n",
      "Uploaded data_for_batch_inference/164, 92 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/3\n",
      "Uploaded data_for_batch_inference/175, 93 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/30\n",
      "Uploaded data_for_batch_inference/186, 94 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/31\n",
      "Uploaded data_for_batch_inference/194, 95 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/32\n",
      "Uploaded data_for_batch_inference/187, 96 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/189, 97 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/33\n",
      "Uploading data_for_batch_inference/34\n",
      "Uploaded data_for_batch_inference/166, 98 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/35\n",
      "Uploaded data_for_batch_inference/17, 99 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/36\n",
      "Uploaded data_for_batch_inference/192, 100 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/37\n",
      "Uploaded data_for_batch_inference/32, 101 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/38\n",
      "Uploaded data_for_batch_inference/190, 102 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/39\n",
      "Uploaded data_for_batch_inference/28, 103 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/4\n",
      "Uploaded data_for_batch_inference/25, 104 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/21, 105 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/40\n",
      "Uploaded data_for_batch_inference/2, 106 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/41\n",
      "Uploading data_for_batch_inference/42\n",
      "Uploaded data_for_batch_inference/22, 107 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/43\n",
      "Uploaded data_for_batch_inference/23, 108 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/44\n",
      "Uploaded data_for_batch_inference/29, 109 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/167, 110 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/27, 111 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/45\n",
      "Uploading data_for_batch_inference/46\n",
      "Uploading data_for_batch_inference/47\n",
      "Uploaded data_for_batch_inference/195, 112 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/48\n",
      "Uploaded data_for_batch_inference/3, 113 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/49\n",
      "Uploaded data_for_batch_inference/165, 114 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/5\n",
      "Uploaded data_for_batch_inference/35, 115 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/50\n",
      "Uploaded data_for_batch_inference/30, 116 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/51\n",
      "Uploaded data_for_batch_inference/24, 117 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/52\n",
      "Uploaded data_for_batch_inference/176, 118 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/53\n",
      "Uploaded data_for_batch_inference/41, 119 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/42, 120 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/54\n",
      "Uploaded data_for_batch_inference/197, 121 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/55\n",
      "Uploading data_for_batch_inference/56\n",
      "Uploaded data_for_batch_inference/38, 122 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/34, 123 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/57\n",
      "Uploaded data_for_batch_inference/33, 124 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/58\n",
      "Uploaded data_for_batch_inference/4, 125 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/59\n",
      "Uploaded data_for_batch_inference/191, 126 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/37, 127 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/6\n",
      "Uploading data_for_batch_inference/60\n",
      "Uploading data_for_batch_inference/61\n",
      "Uploaded data_for_batch_inference/39, 128 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/193, 129 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/62\n",
      "Uploading data_for_batch_inference/63\n",
      "Uploaded data_for_batch_inference/31, 130 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/64\n",
      "Uploaded data_for_batch_inference/56, 131 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/199, 132 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/65\n",
      "Uploaded data_for_batch_inference/6, 133 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/66\n",
      "Uploaded data_for_batch_inference/26, 134 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/67\n",
      "Uploaded data_for_batch_inference/43, 135 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/68\n",
      "Uploading data_for_batch_inference/69\n",
      "Uploaded data_for_batch_inference/46, 136 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/7\n",
      "Uploaded data_for_batch_inference/57, 137 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/70\n",
      "Uploaded data_for_batch_inference/44, 138 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/71\n",
      "Uploaded data_for_batch_inference/55, 139 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/51, 140 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/72\n",
      "Uploaded data_for_batch_inference/36, 141 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/73\n",
      "Uploaded data_for_batch_inference/58, 142 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/74\n",
      "Uploading data_for_batch_inference/75\n",
      "Uploaded data_for_batch_inference/53, 143 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/61, 144 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/76\n",
      "Uploaded data_for_batch_inference/196, 145 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/77\n",
      "Uploaded data_for_batch_inference/19, 146 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/78\n",
      "Uploaded data_for_batch_inference/60, 147 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/79\n",
      "Uploading data_for_batch_inference/8\n",
      "Uploaded data_for_batch_inference/54, 148 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/59, 149 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/80\n",
      "Uploaded data_for_batch_inference/198, 150 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/81\n",
      "Uploaded data_for_batch_inference/40, 151 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/82\n",
      "Uploading data_for_batch_inference/83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data_for_batch_inference/20, 152 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/84\n",
      "Uploaded data_for_batch_inference/69, 153 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/85\n",
      "Uploaded data_for_batch_inference/65, 154 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/86\n",
      "Uploaded data_for_batch_inference/66, 155 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/87\n",
      "Uploaded data_for_batch_inference/71, 156 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/88\n",
      "Uploaded data_for_batch_inference/7, 157 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/89\n",
      "Uploaded data_for_batch_inference/45, 158 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/9\n",
      "Uploaded data_for_batch_inference/68, 159 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/90\n",
      "Uploaded data_for_batch_inference/70, 160 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/91\n",
      "Uploaded data_for_batch_inference/64, 161 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/92\n",
      "Uploaded data_for_batch_inference/86, 162 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/72, 163 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/93\n",
      "Uploading data_for_batch_inference/94\n",
      "Uploaded data_for_batch_inference/73, 164 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/95\n",
      "Uploaded data_for_batch_inference/52, 165 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/96\n",
      "Uploaded data_for_batch_inference/77, 166 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/97\n",
      "Uploaded data_for_batch_inference/48, 167 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/98\n",
      "Uploaded data_for_batch_inference/89, 168 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/75, 169 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/50, 170 files out of an estimated total of 200\n",
      "Uploading data_for_batch_inference/99\n",
      "Uploaded data_for_batch_inference/91, 171 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/76, 172 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/88, 173 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/9, 174 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/83, 175 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/63, 176 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/62, 177 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/87, 178 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/92, 179 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/80, 180 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/94, 181 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/90, 182 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/93, 183 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/78, 184 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/47, 185 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/82, 186 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/67, 187 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/84, 188 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/95, 189 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/74, 190 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/79, 191 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/98, 192 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/97, 193 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/5, 194 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/96, 195 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/99, 196 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/8, 197 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/49, 198 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/81, 199 files out of an estimated total of 200\n",
      "Uploaded data_for_batch_inference/85, 200 files out of an estimated total of 200\n",
      "Uploaded 200 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_b09637c7feff45f793275fa2a9123854"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Register the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'my_dataset_for_batch_inference')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"f519240d-08f3-44bb-8e79-58b8c90810d6\",\n",
       "    \"name\": \"THUCNews_For_Batch_Inference\",\n",
       "    \"version\": 1,\n",
       "    \"description\": \"THUCNews dataset is generated by filtering and filtering historical data of Sina News RSS subscription channel from 2005 to 2011\",\n",
       "    \"workspace\": \"Workspace.create(name='fundamental3', subscription_id='4f455bd0-f95a-4b7d-8d08-078611508e0b', resource_group='fundamental')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'THUCNews_For_Batch_Inference'\n",
    "description = 'THUCNews dataset is generated by filtering and filtering historical data \\\n",
    "of Sina News RSS subscription channel from 2005 to 2011'\n",
    "datastore_path = [DataPath(datastore=datastore, path_on_datastore=path_on_datastore)]\n",
    "data = Dataset.File.from_files(path=datastore_path)\n",
    "data.register(workspace=workspace, name=dataset_name, description=description, create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}